static int CVE_2006_4813_VULN___block_prepare_write(struct inode *inode, struct page unsigned from, unsigned to, get_block_t *get_block) unsigned block_start , block_end ; sector_t block ; int err = 0 ; unsigned blocksize , bbits ; struct buffer_head * bh , * head , * wait [ 2 ] , * * wait_bh = wait ; blocksize = 1 << inode -> i_blkbits; head = page_buffers ( page ); bbits = inode -> i_blkbits; block = ( sector_t ) page -> index << ( PAGE_CACHE_SHIFT - bbits ); for(bh = head, block_start = 0; bh != head || block++, block_start=block_end, bh = bh->b_this_page) block_end = block_start + blocksize; if ( block_end <= from || block_start >= to )  if ( ! buffer_mapped ( bh ) )  err = get_block ( inode , block , bh , 1 ); if ( err )  if ( buffer_new ( bh ) )  if ( PageUptodate ( page ) )  if ( block_end > to || block_start < from )  void * kaddr ; kaddr = kmap_atomic ( page , KM_USER0 ); if ( block_end > to )  memset ( kaddr + to , 0 , block_end - to ); if ( block_start < from )  memset ( kaddr + block_start , 0 , from - block_start ); if ( PageUptodate ( page ) )  if ( ! buffer_uptodate ( bh ) && ! buffer_delay ( bh ) && ( block_start < from || block_end > to ) )  * wait_bh ++ = bh; while ( wait_bh > wait )  if ( ! buffer_uptodate ( * wait_bh ) )  err = - EIO; if ( ! err )  bh = head; block_start = 0; block_end = block_start + blocksize; if ( block_end <= from )  if ( block_start >= to )  if ( buffer_new ( bh ) )  void * kaddr ; kaddr = kmap_atomic ( page , KM_USER0 ); memset ( kaddr + block_start , 0 , bh -> b_size ); kunmap_atomic ( kaddr , KM_USER0 ); block_start = block_end; bh = bh -> b_this_page; while ( bh != head )  