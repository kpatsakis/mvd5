static sg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg) void __user * p = ( void __user * ) arg ; int __user * ip = p ; int result , val , read_only ; Sg_device * sdp ; Sg_fd * sfp ; Sg_request * srp ; if ( ( ! ( sfp = ( Sg_fd * ) filp -> private_data ) ) || ( ! ( sdp = sfp -> parentdp ) ) )  read_only = ( O_RDWR != ( filp -> f_flags & O_ACCMODE ) ); switch ( cmd_in )  if ( atomic_read ( & sdp -> detaching ) )  if ( ! scsi_block_when_processing_errors ( sdp -> device ) )  if ( ! access_ok ( VERIFY_WRITE , p , SZ_SG_IO_HDR ) )  result = sg_new_write ( sfp , filp , p , SZ_SG_IO_HDR , 1 , read_only , 1 , & srp ); static sg_new_write(Sg_fd *sfp, struct file *file, const char __user size_t count, int blocking, int read_only, int Sg_request **o_srp) int k ; Sg_request * srp ; sg_io_hdr_t * hp ; unsigned char cmnd [ SG_MAX_CDB_SIZE ] ; int timeout ; unsigned long ul_timeout ; if ( count < SZ_SG_IO_HDR )  return - EINVAL ; if ( ! access_ok ( VERIFY_READ , buf , count ) )  return - EFAULT ; sfp -> cmd_q = 1; if ( ! ( srp = sg_add_request ( sfp ) ) )  static Sg_request sg_add_request(Sg_fd * sfp) int k ; Sg_request * resp ; Sg_request * rp = sfp -> req_arr ; resp = sfp -> headrp; if ( ! resp )  memset ( rp , 0 , sizeof ( Sg_request ) ); rp -> parentfp = sfp; resp = rp; if ( 0 == sfp -> cmd_q )  resp = NULL; if ( k < SG_MAX_QUEUE )  memset ( rp , 0 , sizeof ( Sg_request ) ); rp -> parentfp = sfp; while ( resp -> nextrp )  resp = resp -> nextrp; resp -> nextrp = rp; resp = rp; resp = NULL; if ( resp )  resp -> nextrp = NULL; resp -> header . duration = jiffies_to_msecs ( jiffies ); return resp ; return - EDOM ; srp -> sg_io_owned = sg_io_owned; hp = & srp -> header; if ( __copy_from_user ( hp , buf , SZ_SG_IO_HDR ) )  return - EFAULT ; if ( hp -> interface_id != 'S' )  return - ENOSYS ; if ( hp -> flags & SG_FLAG_MMAP_IO )  if ( hp -> dxfer_len > sfp -> reserve . bufflen )  return - ENOMEM ; if ( hp -> flags & SG_FLAG_DIRECT_IO )  return - EINVAL ; if ( sg_res_in_use ( sfp ) )  static sg_res_in_use(Sg_fd * sfp) const Sg_request * srp ; for (srp = sfp->headrp; srp; srp = srp->nextrp) if ( srp -> res_used )  return srp ? 1 : 0 ; return - EBUSY ; ul_timeout = msecs_to_jiffies ( srp -> header . timeout ); timeout = ( ul_timeout < INT_MAX ) ? ul_timeout : INT_MAX; if ( ( ! hp -> cmdp ) || ( hp -> cmd_len < 6 ) || ( hp -> cmd_len > sizeof ( cmnd ) ) )  return - EMSGSIZE ; if ( ! access_ok ( VERIFY_READ , hp -> cmdp , hp -> cmd_len ) )  return - EFAULT ; if ( __copy_from_user ( cmnd , hp -> cmdp , hp -> cmd_len ) )  return - EFAULT ; if ( read_only && sg_allow_access ( file , cmnd ) )  static int sg_allow_access(struct file *filp, unsigned char *cmd) struct sg_fd * sfp = filp -> private_data ; if ( sfp -> parentdp -> device -> type == TYPE_SCANNER )  return 0 ; return blk_verify_command ( cmd , filp -> f_mode & FMODE_WRITE ) ; return - EPERM ; k = sg_common_write ( sfp , srp , cmnd , timeout , blocking ); static sg_common_write(Sg_fd * sfp, Sg_request * unsigned char *cmnd, int timeout, int blocking) int k , at_head ; Sg_device * sdp = sfp -> parentdp ; srp -> data . cmd_opcode = cmnd [ 0 ]; k = sg_start_req ( srp , cmnd ); static sg_start_req(Sg_request *srp, unsigned char *cmd) int res ; struct request * rq ; Sg_fd * sfp = srp -> parentfp ; sg_io_hdr_t * hp = & srp -> header ; int dxfer_len = ( int ) hp -> dxfer_len ; int dxfer_dir = hp -> dxfer_direction ; unsigned int iov_count = hp -> iovec_count ; Sg_scatter_hold * req_schp = & srp -> data ; Sg_scatter_hold * rsv_schp = & sfp -> reserve ; struct request_queue * q = sfp -> parentdp -> device -> request_queue ; struct rq_map_data * md , map_data ; int rw = hp -> dxfer_direction == SG_DXFER_TO_DEV ? WRITE : READ ; unsigned char * long_cmdp = NULL ; if ( hp -> cmd_len > BLK_MAX_CDB )  long_cmdp = kzalloc ( hp -> cmd_len , GFP_KERNEL ); if ( ! long_cmdp )  return - ENOMEM ; rq = blk_get_request ( q , rw , GFP_KERNEL ); if ( IS_ERR ( rq ) )  return PTR_ERR ( rq ) ; if ( hp -> cmd_len > BLK_MAX_CDB )  rq -> cmd = long_cmdp; memcpy ( rq -> cmd , cmd , hp -> cmd_len ); rq -> cmd_len = hp -> cmd_len; srp -> rq = rq; rq -> end_io_data = srp; rq -> sense = srp -> sense_b; rq -> retries = SG_DEFAULT_RETRIES; if ( ( dxfer_len <= 0 ) || ( dxfer_dir == SG_DXFER_NONE ) )  return 0 ; if ( sg_allow_dio && hp -> flags & SG_FLAG_DIRECT_IO && dxfer_dir != SG_DXFER_UNKNOWN && ! iov_count && ! sfp -> parentdp -> device -> host -> unchecked_isa_dma && blk_rq_aligned ( q , ( unsigned long ) hp -> dxferp , dxfer_len ) )  md = NULL; md = & map_data; if ( md )  if ( ! sg_res_in_use ( sfp ) && dxfer_len <= rsv_schp -> bufflen )  static sg_res_in_use(Sg_fd * sfp) const Sg_request * srp ; if ( srp -> res_used )  return srp ? 1 : 0 ; res = sg_build_indirect ( req_schp , sfp , dxfer_len ); static sg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size) int sg_tablesize = sfp -> parentdp -> sg_tablesize ; int blk_size = buff_size , order ; gfp_t gfp_mask = GFP_ATOMIC | __GFP_COMP | __GFP_NOWARN ; if ( blk_size < 0 )  return - EFAULT ; blk_size = ALIGN ( blk_size , SG_SECTOR_SZ ); mx_sc_elems = sg_build_sgat ( schp , sfp , sg_tablesize ); static sg_build_sgat(Sg_scatter_hold * schp, const Sg_fd * sfp, int tablesize) int sg_bufflen = tablesize * sizeof ( struct page * ) ; gfp_t gfp_flags = GFP_ATOMIC | __GFP_NOWARN ; schp -> pages = kzalloc ( sg_bufflen , gfp_flags ); if ( ! schp -> pages )  return - ENOMEM ; return tablesize ; if ( mx_sc_elems < 0 )  return mx_sc_elems ; num = scatter_elem_sz; if ( sfp -> low_dma )  gfp_mask |= GFP_DMA; if ( ! capable ( CAP_SYS_ADMIN ) || ! capable ( CAP_SYS_RAWIO ) )  gfp_mask |= __GFP_ZERO; order = get_order ( num ); ret_sz = 1 << ( PAGE_SHIFT + order ); for (k = 0, rem_sz = blk_size; rem_sz > 0 && k < k++, rem_sz -= ret_sz) schp -> pages [ k ] = alloc_pages ( gfp_mask , order ); if ( ! schp -> pages [ k ] )  if ( rem_sz > 0 )  return - ENOMEM ; return 0 ; if ( -- order >= 0 )  return - ENOMEM ; if ( res )  return res ; if ( dxfer_dir == SG_DXFER_TO_FROM_DEV )  md -> from_user = 0; if ( iov_count )  res = import_iovec ( rw , hp -> dxferp , iov_count , 0 , & iov , & i ); if ( res < 0 )  return res ; res = blk_rq_map_user_iov ( q , rq , md , & i , GFP_ATOMIC ); res = blk_rq_map_user ( q , rq , md , hp -> dxferp , hp -> dxfer_len , GFP_ATOMIC ); return res ; if ( k )  return k ; if ( atomic_read ( & sdp -> detaching ) )  return - ENODEV ; return 0 ; if ( k < 0 )  return k ; return count ; if ( result < 0 )  result = wait_event_interruptible ( sfp -> read_wait , ( srp_done ( sfp , srp ) || atomic_read ( & sdp -> detaching ) ) ); static int srp_done(Sg_fd *sfp, Sg_request *srp) int ret ; ret = srp -> done; return ret ; result = get_user ( val , ip ); if ( result )  if ( val < 0 )  if ( val >= MULDIV ( INT_MAX , USER_HZ , HZ ) )  val = MULDIV ( INT_MAX , USER_HZ , HZ ); sfp -> timeout_user = val; sfp -> timeout = MULDIV ( val , HZ , USER_HZ ); result = get_user ( val , ip ); if ( result )  if ( val )  sfp -> low_dma = 1; if ( ( 0 == sfp -> low_dma ) && ( 0 == sg_res_in_use ( sfp ) ) )  static sg_res_in_use(Sg_fd * sfp) const Sg_request * srp ; if ( srp -> res_used )  return srp ? 1 : 0 ; val = ( int ) sfp -> reserve . bufflen; if ( atomic_read ( & sdp -> detaching ) )  sfp -> low_dma = sdp -> device -> host -> unchecked_isa_dma; result = get_user ( val , ip ); if ( result )  sfp -> force_packid = val ? 1 : 0; result = get_user ( val , ip ); if ( result )  if ( val < 0 )  val = min_t ( int , val , max_sectors_bytes ( sdp -> device -> request_queue ) ); static int max_sectors_bytes(struct request_queue *q) unsigned int max_sectors = queue_max_sectors ( q ) ; max_sectors = min_t ( unsigned int , max_sectors , INT_MAX >> 9 ) return max_sectors << 9 ; val = min_t ( int , sfp -> reserve . bufflen , max_sectors_bytes ( sdp -> device -> request_queue ) ); static int max_sectors_bytes(struct request_queue *q) unsigned int max_sectors = queue_max_sectors ( q ) ; max_sectors = min_t ( unsigned int , max_sectors , INT_MAX >> 9 ) return max_sectors << 9 ; result = get_user ( val , ip ); if ( result )  sfp -> cmd_q = val ? 1 : 0; result = get_user ( val , ip ); if ( result )  sfp -> keep_orphan = val; result = get_user ( val , ip ); if ( result )  sfp -> next_cmd_len = ( val > 0 ) ? val : 0; val = ( sdp -> device ? 1 : 0 ); if ( ! access_ok ( VERIFY_WRITE , p , SZ_SG_REQ_INFO * SG_MAX_QUEUE ) )  sg_req_info_t * rinfo ; rinfo = kmalloc ( SZ_SG_REQ_INFO * SG_MAX_QUEUE , GFP_KERNEL ); if ( ! rinfo )  for (srp = sfp->headrp, val = 0; val < ++val, srp = srp ? srp->nextrp : srp) memset ( & rinfo [ val ] , 0 , SZ_SG_REQ_INFO ); 